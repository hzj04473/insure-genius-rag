import PyPDF2
from sentence_transformers import SentenceTransformer
import uuid
import os
import json
import re
from pathlib import Path
from typing import List, Dict, Any
from .pinecone_client import get_index

# 384ì°¨ì› ì„ë² ë”© ëª¨ë¸
model = SentenceTransformer('all-MiniLM-L6-v2')

class EnhancedPDFProcessor:
    def __init__(self):
        self.index = get_index()
        self.documents_path = Path(__file__).parent / 'documents'
        self.insurance_companies = [
            'ì‚¼ì„±í™”ì¬', 'í˜„ëŒ€í•´ìƒ', 'ë©”ë¦¬ì¸ í™”ì¬', 'DBì†í•´ë³´í—˜',
            'ë¡¯ë°ì†í•´ë³´í—˜', 'í•˜ë‚˜ì†í•´ë³´í—˜', 'í¥êµ­í™”ì¬', 'MGì†í•´ë³´í—˜', 'ìºë¡¯ì†í•´ë³´í—˜'
        ]
        
    def extract_text_from_pdf(self, pdf_path: Path) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•œê¸€ ì²˜ë¦¬ ê°œì„ )"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                        text += f"\n\n=== í˜ì´ì§€ {page_num + 1} ===\n"
                        text += page_text
                        
        except Exception as e:
            print(f"PDF ì½ê¸° ì˜¤ë¥˜ ({pdf_path}): {e}")
            return None
        return text
    
    def smart_chunk_text(self, text: str, company_name: str, document_type: str) -> List[Dict]:
        """ë³´í—˜ ì•½ê´€ì— íŠ¹í™”ëœ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹"""
        if not text:
            return []
        
        chunks = []
        
        # ì¡°ë¬¸ë³„ë¡œ ë¶„í•  (ì œXì¡°, ì œXì¥ ë“±)
        article_pattern = r'(ì œ\s*\d+\s*[ì¡°ì¥ì ˆ].*?)(?=ì œ\s*\d+\s*[ì¡°ì¥ì ˆ]|$)'
        articles = re.findall(article_pattern, text, re.DOTALL)
        
        if not articles:
            # ì¡°ë¬¸ì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ì²­í‚¹
            return self.general_chunk_text(text, company_name, document_type)
        
        for i, article in enumerate(articles):
            article = article.strip()
            if len(article) < 50:  # ë„ˆë¬´ ì§§ì€ ì¡°ë¬¸ì€ ìŠ¤í‚µ
                continue
                
            # ì¡°ë¬¸ ì œëª© ì¶”ì¶œ
            title_match = re.match(r'(ì œ\s*\d+\s*[ì¡°ì¥ì ˆ][^:\n]*)', article)
            article_title = title_match.group(1) if title_match else f"ì¡°ë¬¸ {i+1}"
            
            # ê¸´ ì¡°ë¬¸ì€ ì†Œì ˆë¡œ ë‚˜ëˆ„ê¸°
            if len(article) > 1000:
                sub_chunks = self.split_long_article(article, article_title)
                chunks.extend(sub_chunks)
            else:
                chunks.append({
                    'text': article,
                    'title': article_title,
                    'company': company_name,
                    'document_type': document_type,
                    'chunk_type': 'article',
                    'length': len(article)
                })
        
        return chunks
    
    def split_long_article(self, article: str, article_title: str) -> List[Dict]:
        """ê¸´ ì¡°ë¬¸ì„ ì†Œì ˆë¡œ ë¶„í• """
        chunks = []
        
        # í•­ëª©ë³„ë¡œ ë¶„í•  (1., 2., ê°€., ë‚˜. ë“±)
        item_pattern = r'([1-9]\.|[ê°€-í£]\.|â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©)'
        parts = re.split(item_pattern, article)
        
        current_chunk = parts[0]  # ì¡°ë¬¸ ì œëª© ë¶€ë¶„
        
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                item_marker = parts[i]
                item_content = parts[i + 1]
                
                # ì²­í¬ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë¶„í• 
                if len(current_chunk) > 800:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'title': article_title,
                        'company': '',
                        'document_type': '',
                        'chunk_type': 'article_part',
                        'length': len(current_chunk)
                    })
                    current_chunk = f"{article_title}\n{item_marker}{item_content}"
                else:
                    current_chunk += f"{item_marker}{item_content}"
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'title': article_title,
                'company': '',
                'document_type': '',
                'chunk_type': 'article_part',
                'length': len(current_chunk)
            })
        
        return chunks
    
    def general_chunk_text(self, text: str, company_name: str, document_type: str) -> List[Dict]:
        """ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì²­í‚¹"""
        chunks = []
        sentences = re.split(r'[.!?]\s+', text)
        
        current_chunk = ""
        chunk_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # ì²­í¬ í¬ê¸° ì²´í¬
            if len(current_chunk) + len(sentence) > 800 and current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'title': f"ë‚´ìš© {len(chunks) + 1}",
                    'company': company_name,
                    'document_type': document_type,
                    'chunk_type': 'general',
                    'length': len(current_chunk)
                })
                current_chunk = sentence
                chunk_sentences = [sentence]
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                chunk_sentences.append(sentence)
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'title': f"ë‚´ìš© {len(chunks) + 1}",
                'company': company_name,
                'document_type': document_type,
                'chunk_type': 'general',
                'length': len(current_chunk)
            })
        
        return chunks
    
    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©"""
        try:
            embedding = model.encode(text)
            return embedding.tolist()
        except Exception as e:
            print(f"ì„ë² ë”© ì˜¤ë¥˜: {e}")
            return None
    
    def upload_chunks_to_pinecone(self, chunks: List[Dict], namespace: str = "insurance") -> bool:
        """ì²­í¬ë“¤ì„ Pineconeì— ì—…ë¡œë“œ"""
        if not chunks:
            return False
            
        vectors = []
        successful_chunks = 0
        
        for chunk in chunks:
            embedding = self.embed_text(chunk['text'])
            if embedding:
                vector_id = f"{chunk['company']}_{chunk['document_type']}_{str(uuid.uuid4())[:8]}"
                vectors.append({
                    "id": vector_id,
                    "values": embedding,
                    "metadata": {
                        "text": chunk['text'][:1000],  # Pinecone ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ
                        "full_text": chunk['text'],  # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
                        "title": chunk['title'],
                        "company": chunk['company'],
                        "document_type": chunk['document_type'],
                        "chunk_type": chunk['chunk_type'],
                        "length": chunk['length']
                    }
                })
                successful_chunks += 1
        
        if vectors:
            try:
                # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì—…ë¡œë“œ (Pinecone ì œí•œ ê³ ë ¤)
                batch_size = 100
                for i in range(0, len(vectors), batch_size):
                    batch = vectors[i:i + batch_size]
                    self.index.upsert(vectors=batch, namespace=namespace)
                    print(f"ë°°ì¹˜ {i//batch_size + 1} ì—…ë¡œë“œ ì™„ë£Œ: {len(batch)}ê°œ ë²¡í„°")
                
                print(f"âœ… {successful_chunks}ê°œ ì²­í¬ê°€ Pineconeì— ì—…ë¡œë“œë¨")
                return True
            except Exception as e:
                print(f"Pinecone ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
                return False
        return False
    
    def process_company_documents(self, company_name: str) -> bool:
        """íŠ¹ì • ë³´í—˜ì‚¬ì˜ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        company_path = self.documents_path / company_name
        
        if not company_path.exists():
            print(f"âŒ {company_name} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {company_path}")
            return False
        
        pdf_files = list(company_path.glob("*.pdf"))
        if not pdf_files:
            print(f"âŒ {company_name} í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        print(f"ğŸ“‚ {company_name} ì²˜ë¦¬ ì‹œì‘: {len(pdf_files)}ê°œ PDF íŒŒì¼")
        
        total_chunks = 0
        processed_files = 0
        
        for pdf_file in pdf_files:
            print(f"  ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
            
            # ë¬¸ì„œ íƒ€ì… ê²°ì •
            document_type = self.determine_document_type(pdf_file.name)
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = self.extract_text_from_pdf(pdf_file)
            if not text:
                print(f"    âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_file.name}")
                continue
            
            print(f"    ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,} ë¬¸ì")
            
            # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
            chunks = self.smart_chunk_text(text, company_name, document_type)
            print(f"    ğŸ”ª ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
            
            # ê° ì²­í¬ì— íšŒì‚¬ëª…ê³¼ ë¬¸ì„œíƒ€ì… ì„¤ì •
            for chunk in chunks:
                chunk['company'] = company_name
                chunk['document_type'] = document_type
            
            # Pineconeì— ì—…ë¡œë“œ
            if self.upload_chunks_to_pinecone(chunks, namespace=f"insurance_{company_name.replace(' ', '_')}"):
                total_chunks += len(chunks)
                processed_files += 1
                print(f"    âœ… {pdf_file.name} ì²˜ë¦¬ ì™„ë£Œ")
            else:
                print(f"    âŒ {pdf_file.name} ì—…ë¡œë“œ ì‹¤íŒ¨")
        
        print(f"ğŸ‰ {company_name} ì²˜ë¦¬ ì™„ë£Œ: {processed_files}/{len(pdf_files)} íŒŒì¼, ì´ {total_chunks} ì²­í¬")
        return processed_files > 0
    
    def determine_document_type(self, filename: str) -> str:
        """íŒŒì¼ëª…ìœ¼ë¡œ ë¬¸ì„œ íƒ€ì… ê²°ì •"""
        filename_lower = filename.lower()
        
        if 'ì•½ê´€' in filename_lower:
            if 'íŠ¹ì•½' in filename_lower:
                return 'íŠ¹ì•½ì•½ê´€'
            elif 'ìë™ì°¨' in filename_lower:
                return 'ìë™ì°¨ë³´í—˜ì•½ê´€'
            else:
                return 'ë³´í—˜ì•½ê´€'
        elif 'ìƒí’ˆì„¤ëª…ì„œ' in filename_lower:
            return 'ìƒí’ˆì„¤ëª…ì„œ'
        elif 'ì•ˆë‚´ì„œ' in filename_lower:
            return 'ì•ˆë‚´ì„œ'
        else:
            return 'ê¸°íƒ€ë¬¸ì„œ'
    
    def process_all_companies(self) -> Dict[str, bool]:
        """ëª¨ë“  ë³´í—˜ì‚¬ ë¬¸ì„œ ì²˜ë¦¬"""
        results = {}
        
        print("ğŸš€ ëª¨ë“  ë³´í—˜ì‚¬ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        for company in self.insurance_companies:
            print(f"\n{'='*50}")
            print(f"ì²˜ë¦¬ ëŒ€ìƒ: {company}")
            print(f"{'='*50}")
            
            result = self.process_company_documents(company)
            results[company] = result
            
            if result:
                print(f"âœ… {company} ì²˜ë¦¬ ì„±ê³µ")
            else:
                print(f"âŒ {company} ì²˜ë¦¬ ì‹¤íŒ¨")
        
        # ê²°ê³¼ ìš”ì•½
        successful = sum(1 for success in results.values() if success)
        total = len(results)
        
        print(f"\n{'='*50}")
        print(f"ğŸ“Š ì „ì²´ ì²˜ë¦¬ ê²°ê³¼: {successful}/{total} ë³´í—˜ì‚¬ ì„±ê³µ")
        print(f"{'='*50}")
        
        for company, success in results.items():
            status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
            print(f"{company}: {status}")
        
        return results
    
    def search_company_clauses(self, query: str, company_name: str = None, top_k: int = 5) -> List[Dict]:
        """íŠ¹ì • ë³´í—˜ì‚¬ë‚˜ ì „ì²´ì—ì„œ ì•½ê´€ ê²€ìƒ‰"""
        query_embedding = self.embed_text(query)
        if not query_embedding:
            return []
        
        try:
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²°ì •
            if company_name:
                namespace = f"insurance_{company_name.replace(' ', '_')}"
            else:
                namespace = "insurance"
            
            # Pineconeì—ì„œ ê²€ìƒ‰
            results = self.index.query(
                vector=query_embedding,
                top_k=top_k,
                namespace=namespace,
                include_metadata=True,
                filter={"company": company_name} if company_name else None
            )
            
            # ê²°ê³¼ ì •ë¦¬
            similar_clauses = []
            for match in results.get('matches', []):
                metadata = match.get('metadata', {})
                similar_clauses.append({
                    'text': metadata.get('full_text', metadata.get('text', '')),
                    'title': metadata.get('title', ''),
                    'company': metadata.get('company', ''),
                    'document_type': metadata.get('document_type', ''),
                    'chunk_type': metadata.get('chunk_type', ''),
                    'score': match.get('score', 0),
                    'length': metadata.get('length', 0)
                })
            
            return similar_clauses
        
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_company_statistics(self) -> Dict[str, Any]:
        """ê° ë³´í—˜ì‚¬ë³„ ë¬¸ì„œ í†µê³„"""
        stats = {}
        
        for company in self.insurance_companies:
            try:
                namespace = f"insurance_{company.replace(' ', '_')}"
                index_stats = self.index.describe_index_stats()
                
                company_stats = index_stats.get('namespaces', {}).get(namespace, {})
                vector_count = company_stats.get('vector_count', 0)
                
                stats[company] = {
                    'vector_count': vector_count,
                    'has_documents': vector_count > 0
                }
            except Exception as e:
                stats[company] = {
                    'vector_count': 0,
                    'has_documents': False,
                    'error': str(e)
                }
        
        return stats

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ë“¤
def search_similar_clauses(query: str, company_name: str = None, top_k: int = 5) -> List[Dict]:
    """ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    processor = EnhancedPDFProcessor()
    return processor.search_company_clauses(query, company_name, top_k)

def process_pdf_to_pinecone(pdf_path: str, company_name: str, document_type: str = None):
    """ë‹¨ì¼ PDF ì²˜ë¦¬ í•¨ìˆ˜"""
    processor = EnhancedPDFProcessor()
    
    if not document_type:
        document_type = processor.determine_document_type(Path(pdf_path).name)
    
    text = processor.extract_text_from_pdf(Path(pdf_path))
    if not text:
        return False
    
    chunks = processor.smart_chunk_text(text, company_name, document_type)
    
    for chunk in chunks:
        chunk['company'] = company_name
        chunk['document_type'] = document_type
    
    return processor.upload_chunks_to_pinecone(chunks)

# Django Management Commandë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
def initialize_insurance_documents():
    """ëª¨ë“  ë³´í—˜ì‚¬ ë¬¸ì„œë¥¼ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
    processor = EnhancedPDFProcessor()
    return processor.process_all_companies()